{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import matplotlib.path as mplPath\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AURIN_FILE_NAME = 'AURIN_Edu_2016.json'\n",
    "# GEO_CODE_FILE_NAME = ['LGA_2017_VIC.csv', 'LGA_2017_SA.csv', 'LGA_2017_NSW.csv']\n",
    "GEO_CODE_FILE_NAME = ['LGA_2017_NSW.csv']\n",
    "WHOLE_REGION = False\n",
    "VALID_REGION_NUM = 35\n",
    "SPATIAL_FILE_NAME = 'geoinfo.json'\n",
    "# TWITTER_FILE_NAME = ['twitter_mel.json', 'twitter_adelaide.json', 'twitter_syd.json']\n",
    "TWITTER_FILE_NAME = ['twitter_syd.json']\n",
    "\n",
    "\n",
    "VALID_YEAR = ' '\n",
    "corpus_name_list = ['bad-words',\n",
    "                    'body_parts_mild', 'body_parts_medium', 'body_parts_strong', 'body_parts_strongest',\n",
    "                    'gender_identity_mild', 'gender_identity_medium',\n",
    "                    'gender_identity_strong', 'gender_identity_strongest',\n",
    "                    'disability_mild', 'disability_medium',\n",
    "                    'disability_strong', 'disability_strongest',\n",
    "                    'race_ethnicity_mild', 'race_ethnicity_medium',\n",
    "                    'race_ethnicity_strong', 'race_ethnicity_strongest',\n",
    "                    'sexual_reference_mild', 'sexual_reference_medium', 'sexual_reference_strong',\n",
    "                    # 'older_people_mild', 'older_people_medium',\n",
    "                    # 'religious_insults_strong'\n",
    "                    ]\n",
    "\n",
    "WHOLE_CORPUS = corpus_name_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readability_match(file_name):\n",
    "    try:\n",
    "        file = open(file_name, 'r')\n",
    "        json_file = json.load(file)\n",
    "        name_title_match = {}\n",
    "        for item in json_file['selectedAttributes']:\n",
    "            if item['stype'] == 'ratio':\n",
    "                name_title_match[item['name']] = item['title']\n",
    "        file.close()\n",
    "    except:\n",
    "        print(\"Readability match failed\")\n",
    "    return name_title_match\n",
    "\n",
    "\n",
    "def check_belonging(point, contour):\n",
    "    for element in contour:\n",
    "        element = list(element)\n",
    "        crd = np.array(element)  # poly\n",
    "        bbPath = mplPath.Path(crd)\n",
    "        r = 0.00001  # accuracy\n",
    "        isIn = bbPath.contains_point(point, radius=r)\n",
    "        return isIn\n",
    "\n",
    "\n",
    "def get_tweet_coordinates(item):\n",
    "    point = None\n",
    "    if item['doc']['geo']:  # get tweet coordinates\n",
    "        point = item['doc']['geo']['coordinates']\n",
    "    elif item['doc']['coordinates']:\n",
    "        point = item['doc']['coordinates']['coordinates']\n",
    "    elif item['doc']['place']['bounding_box']['coordinates']:\n",
    "        points = item['doc']['place']['bounding_box']['coordinates'][0]\n",
    "        x = []\n",
    "        y = []\n",
    "        [x.append(p[0]) for p in points]\n",
    "        [y.append(p[1]) for p in points]\n",
    "        point = [sum(x)/len(points), sum(y)/len(points)]\n",
    "    return point\n",
    "\n",
    "\n",
    "# def get_valid_code(geoinfo, geo_code):\n",
    "#     valid_code = set()\n",
    "#     for item in geoinfo['features']:\n",
    "#         if item['properties']['lga_code17'] in geo_code.keys():\n",
    "#             valid_code.add(item['properties']['lga_code17'])\n",
    "#     return valid_code\n",
    "\n",
    "\n",
    "def get_valid_code(geoinfo, geo_code, all_valid=False):\n",
    "    valid_code = set()\n",
    "    if all_valid:\n",
    "        for item in geoinfo['features']:\n",
    "            if item['properties']['lga_code17']:\n",
    "                valid_code.add(item['properties']['lga_code17'])\n",
    "    else:\n",
    "        for item in geoinfo['features']:\n",
    "            print(item['properties']['lga_code17'])\n",
    "            if item['properties']['lga_code17'] in geo_code:\n",
    "                valid_code.add(item['properties']['lga_code17'])\n",
    "    return valid_code\n",
    "\n",
    "\n",
    "def get_tweet_region(coordinate, geoinfo, valid_code):\n",
    "    coordinate.reverse()\n",
    "    for polygons in geoinfo['features']:\n",
    "        if polygons['properties']['lga_code17'] in valid_code:\n",
    "            if check_belonging(coordinate, itertools.chain.from_iterable(polygons['geometry']['coordinates'])):\n",
    "                return polygons['properties']['lga_name17']\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_tweet_text(tweet):\n",
    "    text = tweet['doc']['text'].lower()\n",
    "    text = text.replace('-', ' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "def hash_corpus():\n",
    "    corpus_hash = {}\n",
    "    try:\n",
    "        for name in corpus_name_list:\n",
    "            corpus_file = open(name+'.txt', 'r')\n",
    "            for word in corpus_file:\n",
    "                word = word.replace('\\n', '')\n",
    "                pattern = re.compile(r'\\s+' + re.escape(word) + r'(\\s+|\\W+)')\n",
    "                corpus_hash[pattern] = name\n",
    "    except:\n",
    "        print(\"Hashing corpus failed\")\n",
    "    return corpus_hash\n",
    "\n",
    "\n",
    "def get_belonged_corpus(corpus_hash, text):\n",
    "    for pattern, category in corpus_hash.items():\n",
    "        if bool(pattern.search(text)):\n",
    "            return category\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # load files\n",
    "    twitter_json_list = []\n",
    "    # try:\n",
    "    #     for name in TWITTER_FILE_NAME:\n",
    "    #         twitter_file = open(name, 'rb')\n",
    "    #         twitter_json_list.append(json.load(twitter_file))\n",
    "    #         twitter_file.close()\n",
    "    # except:\n",
    "    #     print('Loading Twitter info failed')\n",
    "    try:\n",
    "        aurin_file = open(AURIN_FILE_NAME, 'r')\n",
    "        aurin_info = json.load(aurin_file)\n",
    "        aurin_file.close()\n",
    "\n",
    "        geo_code_list = []\n",
    "        for code in GEO_CODE_FILE_NAME:\n",
    "            geo_code = {}\n",
    "            code_file = open(code, 'r')\n",
    "            reader = csv.reader(code_file)\n",
    "            for row in reader:\n",
    "                geo_code[row[1]] = row[2]\n",
    "            code_file.close()\n",
    "            geo_code_list.append(geo_code)\n",
    "\n",
    "        geo_file = open('geoinfo.json', 'r')\n",
    "        geoinfo = json.load(geo_file)\n",
    "        geo_file.close()\n",
    "\n",
    "        name_title_pair = readability_match('geoinfo_meta.json')\n",
    "        pre_valid_code_list = []\n",
    "        valid_code_list = []\n",
    "        for geo_code in geo_code_list:\n",
    "            pre_valid_code = get_valid_code(geoinfo, geo_code, WHOLE_REGION)\n",
    "            pre_valid_code_list.append(get_valid_code(geoinfo, geo_code, WHOLE_REGION))\n",
    "            sample = random.sample(range(len(pre_valid_code)), VALID_REGION_NUM)\n",
    "            valid_code = []\n",
    "            for item in sample:\n",
    "                pre_valid_code = list(pre_valid_code)\n",
    "                valid_code.append(pre_valid_code[item])\n",
    "            valid_code_list.append(valid_code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # valid_code = get_valid_code(geoinfo, geo_code, WHOLE_REGION)\n",
    "        for code in valid_code_list:\n",
    "            print(\"Valid code: \", len(code))\n",
    "\n",
    "        # generate and store AURIN dataset info\n",
    "        region_info = {}\n",
    "\n",
    "        for valid_code in valid_code_list:\n",
    "            for item in aurin_info['features']:\n",
    "                if item['properties']['lga_code17'] in valid_code:\n",
    "                    region_info[item['properties']['lga_name17']] = {'edu_info': {}, 'word_choice': {}}\n",
    "                    for name, title in name_title_pair.items():\n",
    "                        region_info[item['properties']['lga_name17']]['edu_info'].update({title: item['properties'][name]})\n",
    "\n",
    "        # print(region_info)\n",
    "\n",
    "        corpus_hash = hash_corpus()\n",
    "        corpus_count = {}\n",
    "        corpus_region = {}\n",
    "        region_total_tweets = {}\n",
    "        # print(len(tweets['rows']))\n",
    "\n",
    "        invalid_points = []\n",
    "\n",
    "        j=0\n",
    "        k=0\n",
    "        a=0\n",
    "        b=0\n",
    "        # for tweets in twitter_json_list:\n",
    "        for iter in range(len(TWITTER_FILE_NAME)):\n",
    "            name = TWITTER_FILE_NAME[iter]\n",
    "            valid_code = valid_code_list[iter]\n",
    "            twitter_file = open(name, 'rb')\n",
    "            # tweets = json.load(twitter_file)\n",
    "            # twitter_file.close()\n",
    "            i = 0\n",
    "            for line in twitter_file:\n",
    "                if not line.endswith(b']}\\r\\n') and not line.endswith(b'[\\r\\n'):  # get rid of the first and last line\n",
    "                    if line.endswith(b',\\r\\n'):\n",
    "                        line = line[:-3]\n",
    "                    elif line.endswith(b'}}\\r\\n'):\n",
    "                        line = line[:-2]\n",
    "                    else:\n",
    "                        line = None\n",
    "                else:\n",
    "                    line = None\n",
    "                if line:\n",
    "                    item = json.loads(line.decode(\"utf-8\"))\n",
    "                    if item['doc']['created_at']:\n",
    "                        if bool(re.search(VALID_YEAR, item['doc']['created_at'])):\n",
    "                            point = get_tweet_coordinates(item)  # get Tweet coordinates\n",
    "                            if not point:\n",
    "                                j += 1\n",
    "                            else:\n",
    "                                region_name = get_tweet_region(point, geoinfo, valid_code)\n",
    "                                if not region_name:\n",
    "                                    k += 1\n",
    "                                    invalid_points.append(point)\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        region_total_tweets[region_name] += 1\n",
    "                                    except:\n",
    "                                        region_total_tweets[region_name] = 1\n",
    "                                    text = get_tweet_text(item)\n",
    "                                    a += 1\n",
    "                                    category = get_belonged_corpus(corpus_hash, text)\n",
    "                                    if category:\n",
    "                                        b+=1\n",
    "                                        try:\n",
    "                                            corpus_count[category] += 1  # corpus sum\n",
    "                                        except:\n",
    "                                            corpus_count[category] = 1\n",
    "                                        try:\n",
    "                                            corpus_region[region_name][category] += 1\n",
    "                                            if category != WHOLE_CORPUS:\n",
    "                                                corpus_region[region_name][WHOLE_CORPUS] += 1\n",
    "                                        except:\n",
    "                                            corpus_region[region_name] = {category: 1}\n",
    "                                            if category != WHOLE_CORPUS:\n",
    "                                                try:\n",
    "                                                    corpus_region[region_name][WHOLE_CORPUS] += 1\n",
    "                                                except:\n",
    "                                                    corpus_region[region_name][WHOLE_CORPUS] = 1\n",
    "                i+=1\n",
    "                if i % 10000 == 0:\n",
    "                    print(i, \" Rows Processed\")\n",
    "                if i > 180000:\n",
    "                    break\n",
    "\n",
    "        print('no geo: ',j)\n",
    "        print('no valid region: ', k)\n",
    "        print('has valid region: ', a)\n",
    "        print('all valid: ', b)\n",
    "        print(region_total_tweets)\n",
    "        print(corpus_region)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "    result_json = {'rows': [], 'corpus_count': corpus_count}\n",
    "    for name, value in corpus_region.items():\n",
    "        for key, num in value.items():\n",
    "            value[key] = round(num/region_total_tweets[name]*100, 3)\n",
    "        region_info[name]['word_choice'].update(value)\n",
    "        row = {'region_name': name, 'region_total_tweets': region_total_tweets[name]}\n",
    "        row.update(region_info[name])\n",
    "        print(row)\n",
    "        result_json['rows'].append(row)\n",
    "\n",
    "    result_json = json.dumps(result_json)\n",
    "    print(result_json)\n",
    "\n",
    "    result_corpus_region = open('corpus_region.json', 'w+')\n",
    "    for item in corpus_region:\n",
    "        result_corpus_region.writelines(item)\n",
    "    result_corpus_region.close()\n",
    "\n",
    "    result = open('result.json', 'w+')\n",
    "    for lines in result_json:\n",
    "        result.writelines(lines)\n",
    "    result.close()\n",
    "\n",
    "    corpus_count[WHOLE_CORPUS] = sum(corpus_count.values())\n",
    "    print(corpus_count)\n",
    "\n",
    "    invalid = open('invalid_points.json', 'w+')\n",
    "    invalid_dict = {'points': invalid_points}\n",
    "    invalid_dict = json.dumps(invalid_dict)\n",
    "    invalid.write(invalid_dict)\n",
    "    invalid.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
